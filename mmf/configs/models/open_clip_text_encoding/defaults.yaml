model_config:
  open_clip_text_encoding:
    # Type of bert model
    bert_model_name: bert-base-uncased
    direct_features_input: false
    # Dimension of the embedding finally returned by the modal encoder
    modal_hidden_size: 2048
    # Dimension of the embedding finally returned by the text encoder
    text_hidden_size: 768
    # Used when classification head is activated
    num_labels: 2
    # Number of features extracted out per image
    num_features: 1

    image_encoder:
      type: ViT-L-14
      params:
        pretrained: datacomp_xl_s13b_b90k


    text_encoder:
      type: ViT-L-14
      params:
        bert_model_name: bert-base-uncased
    

    text_encoder_fake:
      type: transformer
      params:
        bert_model_name: bert-base-uncased
        hidden_size: 768
        num_hidden_layers: 12
        num_attention_heads: 12
        output_attentions: false
        output_hidden_states: false


    classifier:
      type: mlp
      params:
        # 2048 + 768 in case of features
        # Modal_Dim * Number of embeddings + Text Dim
        in_dim: 1536
        out_dim: 2
        hidden_dim: 768
        num_layers: 2